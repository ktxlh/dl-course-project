{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Setup"
      ],
      "metadata": {
        "id": "RNXpdQvRQAHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3gnDNYxuRmj",
        "outputId": "1d94428e-33f2-4b05-e417-d6314e84fe18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/DL_Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/DL_Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/mcgill-nlp/bias-bench.git\n",
        "!ls\n",
        "!cd bias-bench && python3 -m pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLJgpZQruVF8",
        "outputId": "754bf5b9-b586-4901-a9f2-9ca04d13d462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DL_Project\n",
            "bias-bench  bias_bench\te2e-coref  test_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.11.2\n",
        "# !pip install torchvision==0.11.3\n",
        "# !pip install torchaudio==0.10.2\n",
        "# !pip install torchdata==0.3.0\n",
        "# !pip install torch==1.10.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IIkxTBEqZtR",
        "outputId": "959754bb-2b29-426b-d576-0181bfe74aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.11.2 in /usr/local/lib/python3.9/dist-packages (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.11.2) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.11.2) (2.27.1)\n",
            "Requirement already satisfied: torch==1.10.2 in /usr/local/lib/python3.9/dist-packages (from torchtext==0.11.2) (1.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.11.2) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.11.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.11.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.11.2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.11.2) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision==0.11.3 in /usr/local/lib/python3.9/dist-packages (0.11.3)\n",
            "Requirement already satisfied: torch==1.10.2 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.11.3) (1.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.11.3) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.11.3) (8.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.2->torchvision==0.11.3) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchaudio==0.10.2 in /usr/local/lib/python3.9/dist-packages (0.10.2)\n",
            "Requirement already satisfied: torch==1.10.2 in /usr/local/lib/python3.9/dist-packages (from torchaudio==0.10.2) (1.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.2->torchaudio==0.10.2) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata==0.3.0 in /usr/local/lib/python3.9/dist-packages (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchdata==0.3.0) (2.27.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata==0.3.0) (1.26.15)\n",
            "Collecting torch==1.11.0\n",
            "  Using cached torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.11.0->torchdata==0.3.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.3.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.3.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.3.0) (2022.12.7)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.2\n",
            "    Uninstalling torch-1.10.2:\n",
            "      Successfully uninstalled torch-1.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.3 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.11.2 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.10.2 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\n",
            "bias-bench 0.1.0 requires torch==1.10.2, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10.2\n",
            "  Using cached torch-1.10.2-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.2) (4.5.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.3.0 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp gdrive/MyDrive/wikipedia-10.txt.zip bias-bench/data\n",
        "# !cp gdrive/MyDrive/wikipedia-2.5.txt.zip bias-bench/data\n",
        "#!cd bias-bench/data && unzip wikipedia-10.txt.zip\n",
        "#!cd bias-bench/data && unzip wikipedia-2.5.txt.zip\n",
        "\n",
        "!ls bias-bench/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL5VsxRvucSq",
        "outputId": "3a513ba1-10fd-4c60-e3d1-a71edc42dd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias_attribute_words.json  seat       wikipedia-10.txt.zip\n",
            "crows\t\t\t   stereoset  wikipedia-2.5.txt.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset.py --model BertForMaskedLM\n",
        "#!ls bias-bench/experiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWtkIxJhugWQ",
        "outputId": "65e41dce-7293-4d44-9197-0d5ddc9e9f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running StereoSet:\n",
            " - persistent_dir: /content/drive/MyDrive/DL_Project/bias-bench\n",
            " - model: BertForMaskedLM\n",
            " - model_name_or_path: bert-base-uncased\n",
            " - batch_size: 1\n",
            " - seed: None\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating intrasentence task.\n",
            "2023-04-27 17:53:35.695839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-27 17:53:38.457217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/stereoset.py\", line 88, in <module>\n",
            "    results = runner()\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/stereoset.py\", line 84, in __call__\n",
            "    intrasentence_bias = self.evaluate_intrasentence()\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/stereoset.py\", line 94, in evaluate_intrasentence\n",
            "    sentence_probabilities = self._likelihood_score()\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/stereoset.py\", line 112, in _likelihood_score\n",
            "    dataset = dataloader.IntrasentenceLoader(\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/dataloader.py\", line 49, in __init__\n",
            "    insertion = self._tokenizer.decode(insertion_tokens[:idx])\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\", line 3290, in decode\n",
            "    token_ids = to_py_obj(token_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2493, in to_py_obj\n",
            "    return [to_py_obj(o) for o in obj]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2493, in <listcomp>\n",
            "    return [to_py_obj(o) for o in obj]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2494, in to_py_obj\n",
            "    elif is_tf_available() and _is_tensorflow(obj):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2475, in _is_tensorflow\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/__init__.py\", line 51, in <module>\n",
            "    from ._api.v2 import compat\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 31, in <module>\n",
            "    from . import compat\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 37, in <module>\n",
            "    from . import v1\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 48, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import lite\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 9, in <module>\n",
            "    from . import experimental\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 8, in <module>\n",
            "    from . import authoring\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.lite.python.authoring.authoring import compatible\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/authoring/authoring.py\", line 43, in <module>\n",
            "    from tensorflow.lite.python import convert\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py\", line 27, in <module>\n",
            "    from tensorflow.lite.python import util\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/util.py\", line 52, in <module>\n",
            "    from jax import xla_computation as _xla_computation\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/__init__.py\", line 157, in <module>\n",
            "    from jax import debug as debug\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/debug.py\", line 14, in <module>\n",
            "    from jax._src.debugging import debug_callback as callback\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/debugging.py\", line 24, in <module>\n",
            "    import jax.numpy as jnp\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/numpy/__init__.py\", line 18, in <module>\n",
            "    from jax.numpy import fft as fft\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/numpy/fft.py\", line 18, in <module>\n",
            "    from jax._src.numpy.fft import (\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/fft.py\", line 21, in <module>\n",
            "    from jax import lax\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/lax/__init__.py\", line 225, in <module>\n",
            "    from jax._src.lax.special import (\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lax/special.py\", line 32, in <module>\n",
            "    from jax._src.lax.control_flow import while_loop\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lax/control_flow/__init__.py\", line 15, in <module>\n",
            "    from jax._src.lax.control_flow.loops import (associative_scan, cummax, cummax_p,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lax/control_flow/loops.py\", line 45, in <module>\n",
            "    from jax._src.lax import windowed_reductions\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lax/windowed_reductions.py\", line 37, in <module>\n",
            "    from jax._src.numpy.ufuncs import logaddexp\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/ufuncs.py\", line 687, in <module>\n",
            "    def heaviside(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/numpy/util.py\", line 159, in wrap\n",
            "    docstr = getattr(fun, \"__doc__\", None)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/numpy/core/_internal.py\", line 826, in _ufunc_doc_signature_formatter\n",
            "    in_args = ', '.join(f'x{i+1}' for i in range(ufunc.nin))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/numpy/core/_internal.py\", line 826, in <genexpr>\n",
            "    in_args = ', '.join(f'x{i+1}' for i in range(ufunc.nin))\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls bias-bench/results/stereoset\n",
        "!python3 bias-bench/experiments/stereoset_evaluation.py --predictions_dir bias-bench/results/stereoset --output_file out1.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In1b_5_PuiNK",
        "outputId": "338c6d92-f487-473e-c2e8-994028f97c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'bias-bench/results/stereoset': No such file or directory\n",
            "Evaluating StereoSet files:\n",
            " - predictions_file: None\n",
            " - predictions_dir: bias-bench/results/stereoset\n",
            " - output_file: out1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tYE6qTNukab",
        "outputId": "83a1f70b-c52c-40e0-f39b-5fd1d1b13259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/stereoset.py\", line 8, in <module>\n",
            "    from bias_bench.model import models\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/model/models.py\", line 6, in <module>\n",
            "    from bias_bench.debias.self_debias.modeling import GPT2Wrapper\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/debias/self_debias/modeling.py\", line 6, in <module>\n",
            "    from transformers import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2694, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 2704, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/albert/__init__.py\", line 35, in <module>\n",
            "    if is_sentencepiece_available():\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/file_utils.py\", line 494, in is_sentencepiece_available\n",
            "    return importlib.util.find_spec(\"sentencepiece\") is not None\n",
            "  File \"/usr/lib/python3.9/importlib/util.py\", line 103, in find_spec\n",
            "    return _find_spec(fullname, parent_path)\n",
            "  File \"<frozen importlib._bootstrap>\", line 925, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1423, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1392, in _get_spec\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls bias-bench/results/stereoset\n",
        "!python3 bias-bench/experiments/stereoset_evaluation.py --predictions_file bias-bench/results/stereoset/stereoset_m-AlbertForMaskedLM_c-albert-base-v2.json --output_file out2.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4njZvOoumVR",
        "outputId": "e2998af5-b911-4de4-8b83-5c39166bd4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'bias-bench/results/stereoset': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/stereoset_evaluation.py\", line 10, in <module>\n",
            "    from bias_bench.benchmark.stereoset import dataloader\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/__init__.py\", line 1, in <module>\n",
            "    from .stereoset import StereoSetRunner\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/bias_bench/benchmark/stereoset/stereoset.py\", line 4, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/__init__.py\", line 197, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "RuntimeError: KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTKuShf6fcbc",
        "outputId": "21c1abb9-68a4-4d59-b9dc-3512b237c4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias-bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 bias-bench/experiments/run_glue.py --h\n",
        "#!python3 bias-bench/experiments/run_glue.py --model_name_or_path albert-base-v2 --output_dir bias_bench --task_name cola --test_file bias_bench/data/stereoset \n",
        "!pip install setuptools==59.5.0\n",
        "!python3 bias-bench/experiments/run_glue.py --model BertForMaskedLM --model_name_or_path bert-base-uncased --output_dir bias_bench --task_name cola"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1asM3GfKjuVi",
        "outputId": "3e4f38ca-9404-4718-b355-6882b2f43edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 59.5.0 which is incompatible.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-59.5.0\n",
            "2023-04-27 18:03:40.418127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-27 18:03:41.434310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/27/2023 18:03:43 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/27/2023 18:03:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bias_bench/runs/Apr27_18-03-43_79c0c3c06781,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=bias_bench,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bias_bench,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/27/2023 18:03:43 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "04/27/2023 18:03:43 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/27/2023 18:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "04/27/2023 18:03:43 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "04/27/2023 18:03:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 614.34it/s]\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:03:43,709 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:03:43,710 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:03:43,954 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:03:43,955 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:03:44,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:03:44,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:03:44,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:03:44,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:03:44,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:03:44,822 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:03:44,823 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "========================================\n",
            "Loading: BertForMaskedLM\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:03:44,980 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:03:44,980 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1427] 2023-04-27 18:03:45,137 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1685] 2023-04-27 18:03:47,666 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1702] 2023-04-27 18:03:47,666 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "========================================\n",
            "04/27/2023 18:03:47 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5effd0eaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "04/27/2023 18:03:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow\n",
            "04/27/2023 18:03:47 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5effd0e820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "04/27/2023 18:03:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow\n",
            "04/27/2023 18:03:47 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f5effd0eaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "04/27/2023 18:03:47 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow\n",
            "[INFO|modelcard.py:460] 2023-04-27 18:03:52,085 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'dataset': {'name': 'GLUE COLA', 'type': 'glue', 'args': 'cola'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DHeg-H55LjC",
        "outputId": "7b0ecd7e-8690-4e0d-b33b-69e52941d1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias-bench  bias_bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/run_glue.py --model BertForMaskedLM --model_name_or_path bert-base-uncased --output_dir test_1 --overwrite_output_dir True --task_name mnli --do_train True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6Y4RLNs6r_C",
        "outputId": "5cfbd724-9b88-47f5-9f67-83d6356b68ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-27 18:33:56.550915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-27 18:33:57.989456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/27/2023 18:33:59 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/27/2023 18:33:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_1/runs/Apr27_18-33-59_79c0c3c06781,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=test_1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_1,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/27/2023 18:33:59 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "04/27/2023 18:33:59 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "04/27/2023 18:34:00 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/27/2023 18:34:00 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/MNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpidsvpp4q\n",
            "Downloading: 100% 313M/313M [00:03<00:00, 90.6MB/s]\n",
            "04/27/2023 18:34:03 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/MNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "04/27/2023 18:34:03 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\n",
            "04/27/2023 18:34:03 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "04/27/2023 18:34:04 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "04/27/2023 18:34:14 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "04/27/2023 18:34:14 - INFO - datasets.builder - Generating split train\n",
            "04/27/2023 18:34:42 - INFO - datasets.builder - Generating split validation_matched\n",
            "04/27/2023 18:34:43 - INFO - datasets.builder - Generating split validation_mismatched\n",
            "04/27/2023 18:34:43 - INFO - datasets.builder - Generating split test_matched\n",
            "04/27/2023 18:34:44 - INFO - datasets.builder - Generating split test_mismatched\n",
            "04/27/2023 18:34:44 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 5/5 [00:00<00:00, 822.96it/s]\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:34:44,996 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:34:44,997 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"mnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:34:45,246 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:34:45,246 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:34:45,957 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:34:45,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:34:45,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:34:45,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1771] 2023-04-27 18:34:45,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:34:46,076 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:34:46,077 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "========================================\n",
            "Loading: BertForMaskedLM\n",
            "[INFO|configuration_utils.py:644] 2023-04-27 18:34:46,216 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:680] 2023-04-27 18:34:46,217 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1427] 2023-04-27 18:34:46,338 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1685] 2023-04-27 18:34:48,129 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1702] 2023-04-27 18:34:48,129 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "========================================\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/run_glue.py\", line 719, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/run_glue.py\", line 488, in main\n",
            "    logger.warning(\n",
            "Message: \"Your model seems to have been trained with labels, but they don't match the dataset: \"\n",
            "Arguments: (\"model labels: ['label_0', 'label_1'], dataset labels: ['contradiction', 'entailment', 'neutral'].\\nIgnoring the model labels as a result.\",)\n",
            "04/27/2023 18:34:48 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0ce59e94c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/393 [00:00<?, ?ba/s]04/27/2023 18:34:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 393/393 [01:26<00:00,  4.52ba/s]\n",
            "04/27/2023 18:36:15 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0ce54101f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]04/27/2023 18:36:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  6.09ba/s]\n",
            "04/27/2023 18:36:16 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0ce54101f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]04/27/2023 18:36:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3eb13b9046685257.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  6.10ba/s]\n",
            "04/27/2023 18:36:18 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0ce54101f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]04/27/2023 18:36:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23b8c1e9392456de.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:01<00:00,  5.36ba/s]\n",
            "04/27/2023 18:36:20 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f0ce54101f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/10 [00:00<?, ?ba/s]04/27/2023 18:36:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1a3d1fa7bc8960a9.arrow\n",
            "Running tokenizer on dataset: 100% 10/10 [00:02<00:00,  4.99ba/s]\n",
            "04/27/2023 18:36:22 - INFO - __main__ - Sample 354785 of the training set: {'premise': 'The town was little more than a single main street, bordered by shops and houses.', 'hypothesis': 'The small town had little more that one street that was bordered by shops, houses and a train station.', 'label': 1, 'idx': 354785, 'input_ids': [101, 1996, 2237, 2001, 2210, 2062, 2084, 1037, 2309, 2364, 2395, 1010, 11356, 2011, 7340, 1998, 3506, 1012, 102, 1996, 2235, 2237, 2018, 2210, 2062, 2008, 2028, 2395, 2008, 2001, 11356, 2011, 7340, 1010, 3506, 1998, 1037, 3345, 2276, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/27/2023 18:36:22 - INFO - __main__ - Sample 388323 of the training set: {'premise': \"Below Jesus' feet is a Latin in?\\xadscription that suggests that the tympanum was the work of one man, Gislebertus (Gilbert).\", 'hypothesis': 'Gisleburtus destroyed the tympanum of Jesus.', 'label': 2, 'idx': 388323, 'input_ids': [101, 2917, 4441, 1005, 2519, 2003, 1037, 3763, 1999, 1029, 5896, 3258, 2008, 6083, 2008, 1996, 5939, 8737, 27975, 2001, 1996, 2147, 1997, 2028, 2158, 1010, 21025, 25016, 8296, 2271, 1006, 7664, 1007, 1012, 102, 21025, 25016, 8569, 5339, 2271, 3908, 1996, 5939, 8737, 27975, 1997, 4441, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/27/2023 18:36:22 - INFO - __main__ - Sample 285929 of the training set: {'premise': \"A history of mad-cow disease by veteran science writer Richard Rhodes ( The Making of the Atomic Bomb ). Critics praise Rhodes' lucid explication of complex virology as well as his range--the book includes digressions on political and literary themes and a section on the scandal surrounding a Nobel Prize-winning expert on the disease who recently pleaded guilty to molesting a little boy.\", 'hypothesis': \"Critics appreciated the book of the scientist Richard Rhodes, which succeeded in explaining very carefully the mad-cow disease and other digressions, but it hasn't won any prize yet.\", 'label': 1, 'idx': 285929, 'input_ids': [101, 1037, 2381, 1997, 5506, 1011, 11190, 4295, 2011, 8003, 2671, 3213, 2957, 10588, 1006, 1996, 2437, 1997, 1996, 9593, 5968, 1007, 1012, 4401, 8489, 10588, 1005, 12776, 3593, 4654, 21557, 1997, 3375, 6819, 13153, 15707, 2004, 2092, 2004, 2010, 2846, 1011, 1011, 1996, 2338, 2950, 10667, 8303, 8496, 2006, 2576, 1998, 4706, 6991, 1998, 1037, 2930, 2006, 1996, 9446, 4193, 1037, 10501, 3396, 1011, 3045, 6739, 2006, 1996, 4295, 2040, 3728, 12254, 5905, 2000, 16709, 16643, 3070, 1037, 2210, 2879, 1012, 102, 4401, 12315, 1996, 2338, 1997, 1996, 7155, 2957, 10588, 1010, 2029, 4594, 1999, 9990, 2200, 5362, 1996, 5506, 1011, 11190, 4295, 1998, 2060, 10667, 8303, 8496, 1010, 2021, 2009, 8440, 1005, 1056, 2180, 2151, 3396, 2664, 1012, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "trainer initialized\n",
            "[INFO|trainer.py:553] 2023-04-27 18:36:28,333 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: premise, hypothesis, idx.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1244] 2023-04-27 18:36:28,355 >> ***** Running training *****\n",
            "[INFO|trainer.py:1245] 2023-04-27 18:36:28,355 >>   Num examples = 392702\n",
            "[INFO|trainer.py:1246] 2023-04-27 18:36:28,355 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1247] 2023-04-27 18:36:28,355 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1248] 2023-04-27 18:36:28,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1249] 2023-04-27 18:36:28,356 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1250] 2023-04-27 18:36:28,356 >>   Total optimization steps = 147264\n",
            "  0% 0/147264 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/run_glue.py\", line 719, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/DL_Project/bias-bench/experiments/run_glue.py\", line 621, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1365, in train\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1940, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1972, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py\", line 1363, in forward\n",
            "    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\", line 1150, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 2846, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "ValueError: Expected input batch_size (1024) to match target batch_size (8).\n",
            "  0% 0/147264 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/run_glue.py --h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgq1KuSN6I7o",
        "outputId": "3d4b18c8-5fd9-42f0-d073-3076f0e35526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-27 18:10:32.453964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-27 18:10:33.480735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: run_glue.py\n",
            "       [-h]\n",
            "       --model_name_or_path\n",
            "       MODEL_NAME_OR_PATH\n",
            "       [--config_name CONFIG_NAME]\n",
            "       [--tokenizer_name TOKENIZER_NAME]\n",
            "       [--cache_dir CACHE_DIR]\n",
            "       [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "       [--no_use_fast_tokenizer]\n",
            "       [--model_revision MODEL_REVISION]\n",
            "       [--use_auth_token [USE_AUTH_TOKEN]]\n",
            "       [--model MODEL]\n",
            "       [--bias_direction BIAS_DIRECTION]\n",
            "       [--projection_matrix PROJECTION_MATRIX]\n",
            "       [--task_name TASK_NAME]\n",
            "       [--dataset_name DATASET_NAME]\n",
            "       [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "       [--max_seq_length MAX_SEQ_LENGTH]\n",
            "       [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "       [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "       [--no_pad_to_max_length]\n",
            "       [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "       [--max_eval_samples MAX_EVAL_SAMPLES]\n",
            "       [--max_predict_samples MAX_PREDICT_SAMPLES]\n",
            "       [--train_file TRAIN_FILE]\n",
            "       [--validation_file VALIDATION_FILE]\n",
            "       [--test_file TEST_FILE]\n",
            "       [--persistent_dir PERSISTENT_DIR]\n",
            "       --output_dir\n",
            "       OUTPUT_DIR\n",
            "       [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "       [--do_train [DO_TRAIN]]\n",
            "       [--do_eval [DO_EVAL]]\n",
            "       [--do_predict [DO_PREDICT]]\n",
            "       [--evaluation_strategy {no,steps,epoch}]\n",
            "       [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "       [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "       [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "       [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "       [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "       [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "       [--learning_rate LEARNING_RATE]\n",
            "       [--weight_decay WEIGHT_DECAY]\n",
            "       [--adam_beta1 ADAM_BETA1]\n",
            "       [--adam_beta2 ADAM_BETA2]\n",
            "       [--adam_epsilon ADAM_EPSILON]\n",
            "       [--max_grad_norm MAX_GRAD_NORM]\n",
            "       [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "       [--max_steps MAX_STEPS]\n",
            "       [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "       [--warmup_ratio WARMUP_RATIO]\n",
            "       [--warmup_steps WARMUP_STEPS]\n",
            "       [--log_level {debug,info,warning,error,critical,passive}]\n",
            "       [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
            "       [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
            "       [--no_log_on_each_node]\n",
            "       [--logging_dir LOGGING_DIR]\n",
            "       [--logging_strategy {no,steps,epoch}]\n",
            "       [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "       [--logging_steps LOGGING_STEPS]\n",
            "       [--logging_nan_inf_filter LOGGING_NAN_INF_FILTER]\n",
            "       [--save_strategy {no,steps,epoch}]\n",
            "       [--save_steps SAVE_STEPS]\n",
            "       [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "       [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "       [--no_cuda [NO_CUDA]]\n",
            "       [--seed SEED]\n",
            "       [--bf16 [BF16]]\n",
            "       [--fp16 [FP16]]\n",
            "       [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "       [--half_precision_backend {auto,amp,apex}]\n",
            "       [--bf16_full_eval [BF16_FULL_EVAL]]\n",
            "       [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "       [--tf32 [TF32]]\n",
            "       [--local_rank LOCAL_RANK]\n",
            "       [--xpu_backend {mpi,ccl}]\n",
            "       [--tpu_num_cores TPU_NUM_CORES]\n",
            "       [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
            "       [--debug DEBUG]\n",
            "       [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "       [--eval_steps EVAL_STEPS]\n",
            "       [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "       [--past_index PAST_INDEX]\n",
            "       [--run_name RUN_NAME]\n",
            "       [--disable_tqdm DISABLE_TQDM]\n",
            "       [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "       [--no_remove_unused_columns]\n",
            "       [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "       [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "       [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "       [--greater_is_better GREATER_IS_BETTER]\n",
            "       [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "       [--sharded_ddp SHARDED_DDP]\n",
            "       [--deepspeed DEEPSPEED]\n",
            "       [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "       [--optim {adamw_hf,adamw_torch,adamw_apex_fused,adafactor}]\n",
            "       [--adafactor [ADAFACTOR]]\n",
            "       [--group_by_length [GROUP_BY_LENGTH]]\n",
            "       [--length_column_name LENGTH_COLUMN_NAME]\n",
            "       [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "       [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "       [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
            "       [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "       [--no_dataloader_pin_memory]\n",
            "       [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "       [--no_skip_memory_metrics]\n",
            "       [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "       [--push_to_hub [PUSH_TO_HUB]]\n",
            "       [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "       [--hub_model_id HUB_MODEL_ID]\n",
            "       [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
            "       [--hub_token HUB_TOKEN]\n",
            "       [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
            "       [--fp16_backend {auto,amp,apex}]\n",
            "       [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "       [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "       [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
            "       [--mp_parameters MP_PARAMETERS]\n",
            "run_glue.py: error: ambiguous option: --h could match --help, --half_precision_backend, --hub_model_id, --hub_strategy, --hub_token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coreference"
      ],
      "metadata": {
        "id": "L58AoXwiX37_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/kentonl/e2e-coref.git\n",
        "!ls\n",
        "%cd e2e-coref/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFmzBqxhX58y",
        "outputId": "753cc476-9207-4d42-9d92-1a4e2041fa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias-bench  bias_bench\te2e-coref  test_1\n",
            "/content/drive/MyDrive/DL_Project/e2e-coref\n",
            "cache_elmo.py\t\tdemo.py\t\t      metrics.py   requirements.txt\n",
            "conll.py\t\tevaluate.py\t      minimize.py  setup_all.sh\n",
            "continuous_evaluate.py\texperiments.conf      predict.py   setup_training.sh\n",
            "coref_kernels.cc\tfilter_embeddings.py  ps.py\t   train.py\n",
            "coref_model.py\t\tget_char_vocab.py     __pycache__  util.py\n",
            "coref_ops.py\t\tLICENSE\t\t      README.md    worker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n",
        "!pip install tensorflow==1.15.0 # it was greater than or equal to 1.13.1, latest version isn't working os i just picked one\n",
        "!pip install tensorflow-hub\n",
        "!pip install h5py\n",
        "!pip install nltk\n",
        "!pip install pyhocon\n",
        "!pip install scipy\n",
        "!pip install sklearn==1.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnhenIeBYwns",
        "outputId": "552837a4-087f-49d9-96e1-1346f250e7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.22.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyhocon in /usr/local/lib/python3.10/dist-packages (0.3.60)\n",
            "Requirement already satisfied: pyparsing<4,>=2 in /usr/local/lib/python3.10/dist-packages (from pyhocon) (3.0.9)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn==1.2.0 (from versions: 0.0, 0.0.post1, 0.0.post2, 0.0.post4)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn==1.2.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Ga2xdA3_Dg",
        "outputId": "5eed0a72-cb3e-4578-c012-d092d343e5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.0.1)\n",
            "Collecting pip\n",
            "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: pip 23.0.1\n",
            "    Uninstalling pip-23.0.1:\n",
            "      Successfully uninstalled pip-23.0.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed pip-23.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!python demo.py final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr7NMpqRX6Bp",
        "outputId": "fe78821c-6c56-4958-bcff-d56be2b94c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/DL_Project/e2e-coref/demo.py\", line 7, in <module>\n",
            "    import coref_model as cm\n",
            "  File \"/content/drive/MyDrive/DL_Project/e2e-coref/coref_model.py\", line 16, in <module>\n",
            "    import util\n",
            "  File \"/content/drive/MyDrive/DL_Project/e2e-coref/util.py\", line 211, in <module>\n",
            "    class CustomLSTMCell(tf.contrib.rnn.RNNCell):\n",
            "AttributeError: module 'tensorflow' has no attribute 'contrib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "EWUl4iLnDE2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Jpw3ISDeyn",
        "outputId": "6f30f2c5-4bf0-4d43-f42d-536627d2badd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dl_finetuning.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDUBOUCfDHJY",
        "outputId": "f59f2731-5ca1-437a-c51b-ac93a977dc6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-01 17:35:00.295810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 5/5 [00:00<00:00, 31.23it/s]\n",
            ">>> Perplexity: 60.91\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1319: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
            "100% 5/5 [00:00<00:00, 50.73it/s]\n",
            ">>> Perplexity: 1.03\n",
            "0\n",
            "0\n",
            "0\n",
            "tensor([[749, 329,   4,  44, 341, 600, 395, 176, 103, 545, 267, 497, 195, 103,\n",
            "          68, 103, 183,  33, 511, 183, 489, 146, 572, 767, 183],\n",
            "        [429, 400, 434, 455,  69, 401, 294, 390, 546, 103, 294, 103, 144, 426,\n",
            "         690, 426, 183, 183, 103, 161, 304, 487, 163, 614, 672],\n",
            "        [294, 141, 743, 441, 571, 560, 206, 103, 103, 103, 149, 456, 677, 317,\n",
            "         568, 590, 130,  51, 710, 426, 107, 103, 245, 493, 512],\n",
            "        [358, 662, 694, 103, 288, 570, 642, 156, 103, 760, 699, 700, 103, 151,\n",
            "         617, 103, 700, 103, 351, 183, 371, 163, 103, 183, 560],\n",
            "        [513, 103,   5, 606, 578, 186, 338, 545, 391, 629,  47, 103,  67, 235,\n",
            "         436, 752, 103, 752, 350, 183, 103, 103, 426, 572, 665],\n",
            "        [103, 341, 103, 142, 634, 299,  33, 395, 103,  55, 571, 121, 103, 429,\n",
            "         103, 505, 505, 303, 426, 502,  83, 396, 103, 623, 446],\n",
            "        [422, 290, 463, 385,  86, 178, 682, 243,  46, 655, 103, 539, 380, 103,\n",
            "         341,  69, 482, 283, 639, 614, 315, 183, 446, 183, 426],\n",
            "        [190, 387, 569, 571, 251, 599, 723, 269, 739, 103, 387, 637, 481, 463,\n",
            "         184,   4, 103, 750, 275, 218, 481, 103, 446, 134, 339],\n",
            "        [ 48, 657, 715, 747, 539, 593, 303, 545, 103, 601, 718, 446, 738, 103,\n",
            "         103, 430,   8, 539, 116, 174, 571, 103, 103, 103, 396],\n",
            "        [282, 419, 341,  35, 103, 668,  42, 206, 224, 213, 103, 488, 419,  35,\n",
            "         103, 439, 411, 325, 411, 659, 103, 587,  71, 503, 503],\n",
            "        [139, 189, 112, 103,  26, 103, 121,  81, 448, 119, 103, 629, 633, 103,\n",
            "         121, 144, 364,  29, 331, 381, 103, 711, 672, 426, 249],\n",
            "        [290, 103, 103, 691, 610, 121, 612, 572, 349,  29, 274,   2,  45, 103,\n",
            "         207, 688, 443, 351, 514, 396, 554, 103, 278, 598, 298],\n",
            "        [185, 274, 410,  46, 658, 528, 525, 281, 301, 103, 319, 103, 103,   0,\n",
            "         140, 260, 103, 103, 599, 103, 341, 554, 446, 567, 446],\n",
            "        [119, 760, 103, 103, 399, 622, 162, 134, 121,  46, 544, 121, 700, 103,\n",
            "         681,  62, 113, 742, 113, 725, 337, 103,  83, 323, 539],\n",
            "        [151, 103, 103, 111,  48, 302, 170, 365, 103, 733, 144, 342, 712, 103,\n",
            "          46, 103, 353, 183, 101, 183, 103, 183, 505, 183, 664],\n",
            "        [139, 618, 655, 294, 121, 103, 549, 678, 750, 655, 365, 656, 501, 290,\n",
            "         103, 103, 153, 615, 449, 548, 747, 614, 339, 163, 446]],\n",
            "       device='cuda:0')\n",
            "distill time\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tensor(12.7700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(8.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(6.9141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "100% 5/5 [00:00<00:00, 27.50it/s]\n",
            ">>> Perplexity: 36.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing New Models on StereoSet"
      ],
      "metadata": {
        "id": "kUk01IU3XwBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/mcgill-nlp/bias-bench.git\n",
        "!ls\n",
        "!cd bias-bench && python3 -m pip install -e .\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmB1i9IhZhiq",
        "outputId": "bce6c58b-2cde-4ff9-fa51-6c24c9874646"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "antistereotypes_labels.txt\t    distill_dataset_models\n",
            "antistereotypes_snippet_labels.txt  dl_finetuning.py\n",
            "antistereotypes_snippet.txt\t    logs\n",
            "antistereotypes_text.txt\t    models\n",
            "bert-base-uncased-finetuned\t    Raj_CS7643.ipynb\n",
            "bias-bench\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/DL_Project/bias-bench\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.11.0 (from bias-bench==0.1.0)\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.16.2 (from bias-bench==0.1.0)\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.3 (from bias-bench==0.1.0)\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.2 (from bias-bench==0.1.0)\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.7.0 (from bias-bench==0.1.0)\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.18.3 (from bias-bench==0.1.0)\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.5.1 (from bias-bench==0.1.0)\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.5.1->bias-bench==0.1.0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.5.1->bias-bench==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (9.0.0)\n",
            "Collecting dill (from datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (4.65.0)\n",
            "Collecting xxhash (from datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (2023.4.0)\n",
            "Collecting aiohttp (from datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (23.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->bias-bench==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->bias-bench==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->bias-bench==0.1.0) (3.12.0)\n",
            "Collecting sacremoses (from transformers==4.16.2->bias-bench==0.1.0)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1 (from transformers==4.16.2->bias-bench==0.1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==1.18.3->bias-bench==0.1.0)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->bias-bench==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->bias-bench==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->bias-bench==0.1.0) (1.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=49a8a2bf2156bb6794522876fb20aa1d777aa8d9512bc2932c2f4aeaec08a9e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, xxhash, torch, scipy, sacremoses, nltk, multidict, frozenlist, dill, async-timeout, yarl, scikit-learn, multiprocess, huggingface-hub, aiosignal, accelerate, transformers, aiohttp, datasets, bias-bench\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Running setup.py develop for bias-bench\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.5.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bias-bench-0.1.0 datasets-1.18.3 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 nltk-3.7 sacremoses-0.0.53 scikit-learn-1.0.2 scipy-1.7.3 tokenizers-0.13.3 torch-1.11.0 transformers-4.16.2 xxhash-3.2.0 yarl-1.9.2\n",
            "antistereotypes_labels.txt\t    distill_dataset_models\n",
            "antistereotypes_snippet_labels.txt  dl_finetuning.py\n",
            "antistereotypes_snippet.txt\t    logs\n",
            "antistereotypes_text.txt\t    models\n",
            "bert-base-uncased-finetuned\t    Raj_CS7643.ipynb\n",
            "bias-bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset.py --model FineTunedBert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCbirAHb58O",
        "outputId": "59d6c5d9-9c42-47ac-8fb2-f86d52c648fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running StereoSet:\n",
            " - persistent_dir: /content/drive/MyDrive/DL_Project/bias-bench\n",
            " - model: FineTunedBert\n",
            " - model_name_or_path: bert-base-uncased\n",
            " - batch_size: 1\n",
            " - seed: None\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 28.8kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 581kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 541kB/s] \n",
            "Downloading: 100% 455k/455k [00:00<00:00, 20.1MB/s]\n",
            "Evaluating intrasentence task.\n",
            "2023-05-01 17:52:12.206469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-01 17:52:14.584852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "100% 24464/24464 [04:44<00:00, 86.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls bias-bench/results/stereoset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUPoCAydeOxU",
        "outputId": "669b8fd6-11bb-4f03-fadb-b2936ca3831d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stereoset_m-BertDistillDataset_c-bert-base-uncased.json\n",
            "stereoset_m-FineTunedBert_c-bert-base-uncased.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset_evaluation.py --predictions_file bias-bench/results/stereoset/stereoset_m-FineTunedBert_c-bert-base-uncased.json --output_file out2.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo_voufDdvEv",
        "outputId": "b4973e98-4caf-495c-f2b7-ed672adb022c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating StereoSet files:\n",
            " - predictions_file: bias-bench/results/stereoset/stereoset_m-FineTunedBert_c-bert-base-uncased.json\n",
            " - predictions_dir: None\n",
            " - output_file: out2.txt\n",
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 2313.0\n",
            "\t\tLM Score: 82.60934589823894\n",
            "\t\tSS Score: 58.52027713342057\n",
            "\t\tICAT Score: 68.53225548096702\n",
            "\tprofession\n",
            "\t\tCount: 7194.0\n",
            "\t\tLM Score: 80.373824359429\n",
            "\t\tSS Score: 54.62181750975015\n",
            "\t\tICAT Score: 72.94436138442914\n",
            "\trace\n",
            "\t\tCount: 8928.0\n",
            "\t\tLM Score: 80.64857474029597\n",
            "\t\tSS Score: 55.76354558485167\n",
            "\t\tICAT Score: 71.35214000291572\n",
            "\treligion\n",
            "\t\tCount: 741.0\n",
            "\t\tLM Score: 77.56465149401747\n",
            "\t\tSS Score: 60.96753327009167\n",
            "\t\tICAT Score: 60.55079357714342\n",
            "\toverall\n",
            "\t\tCount: 6392.0\n",
            "\t\tLM Score: 80.67477429089001\n",
            "\t\tSS Score: 55.87421628264131\n",
            "\t\tICAT Score: 71.19675283613084\n",
            "overall\n",
            "\tCount: 6392.0\n",
            "\tLM Score: 80.67477429089001\n",
            "\tSS Score: 55.87421628264131\n",
            "\tICAT Score: 71.19675283613084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset.py --model BertDistillDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCnp8aBpd7eQ",
        "outputId": "6215f442-c0d2-48a0-b3dc-c0ff2da71933"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running StereoSet:\n",
            " - persistent_dir: /content/drive/MyDrive/DL_Project/bias-bench\n",
            " - model: BertDistillDataset\n",
            " - model_name_or_path: bert-base-uncased\n",
            " - batch_size: 1\n",
            " - seed: None\n",
            "Evaluating intrasentence task.\n",
            "2023-05-01 18:01:34.866482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-01 18:01:36.551146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "100% 24464/24464 [04:44<00:00, 86.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 bias-bench/experiments/stereoset_evaluation.py --predictions_file bias-bench/results/stereoset/stereoset_m-BertDistillDataset_c-bert-base-uncased.json --output_file out3.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02oUgCjld9Wv",
        "outputId": "ba8d508c-97d1-4fea-e49b-fcb5c6f9dd22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating StereoSet files:\n",
            " - predictions_file: bias-bench/results/stereoset/stereoset_m-BertDistillDataset_c-bert-base-uncased.json\n",
            " - predictions_dir: None\n",
            " - output_file: out3.txt\n",
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 2313.0\n",
            "\t\tLM Score: 85.43956329407355\n",
            "\t\tSS Score: 59.36012387764013\n",
            "\t\tICAT Score: 69.4450653643935\n",
            "\tprofession\n",
            "\t\tCount: 7194.0\n",
            "\t\tLM Score: 84.08559490121573\n",
            "\t\tSS Score: 57.663192186923595\n",
            "\t\tICAT Score: 71.19831342361935\n",
            "\trace\n",
            "\t\tCount: 8928.0\n",
            "\t\tLM Score: 83.96525475962655\n",
            "\t\tSS Score: 58.78145740459302\n",
            "\t\tICAT Score: 69.21850859687731\n",
            "\treligion\n",
            "\t\tCount: 741.0\n",
            "\t\tLM Score: 83.99737676094122\n",
            "\t\tSS Score: 58.52373901749752\n",
            "\t\tICAT Score: 69.67794240764775\n",
            "\toverall\n",
            "\t\tCount: 6392.0\n",
            "\t\tLM Score: 84.19396949842104\n",
            "\t\tSS Score: 58.427724591028166\n",
            "\t\tICAT Score: 70.00269775525867\n",
            "overall\n",
            "\tCount: 6392.0\n",
            "\tLM Score: 84.19396949842104\n",
            "\tSS Score: 58.427724591028166\n",
            "\tICAT Score: 70.00269775525867\n"
          ]
        }
      ]
    }
  ]
}